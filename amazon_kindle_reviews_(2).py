# -*- coding: utf-8 -*-
"""Amazon_Kindle_Reviews (2).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RUWeie0y3shRJetOWFdqkK4fApNnBUBo

**Google Drive Mount**
"""

from google.colab import drive
drive.mount("/content/gdrive")

"""**Imports and Load Data**"""

import numpy as np  # linear algebra
import pandas as pd # data reading, processing in different, like text ,CSV or pdf file

df = pd.read_csv(r'/content/gdrive/MyDrive/Project/kindle_reviews.csv') # data set read

df.head(10)   # here i checked data after read by using head function also we pass perameter in integer value

"""**Exploratory Data Analysis**"""

len(df)   # here i checked the lenght of dataSet

df.isnull().sum()   # for missing value , it is necessary steps in preprocessing because if data set have missing value we can not tarin our ML Model

#So above you can see in " ReviewerName ","reviewerName" column have many missing value , now need to remove this missing value
#drop the rows where there are no reviews
df.dropna(subset = ['reviewText'], inplace = True)
df.dropna(subset = ['reviewerName'], inplace = True)

df.isnull().sum()

df.columns # check for column

df.dtypes # for check datatype

df.info()  # for checking dataSet information

#changing the reviewTime column to be of datetime type
df.reviewTime = pd.to_datetime(df.reviewTime)

#creating a column with just the year so in this way we can also easily find review of each years
df['Year'] = df.reviewTime.dt.year

#show 7 top value of each row
df.head(7)

import matplotlib.pyplot as plt  # for graphically representation
df.Year.value_counts().sort_index().plot(kind = 'bar')
plt.title('Number of Reviews per Year')
plt.xlabel('Year')
plt.ylabel('Number of Reviews')
plt.show()

df.overall.value_counts().plot(kind = 'pie')
plt.title('Number of Good Ratings vs Bad Ratings')
plt.xlabel('Rating Scales')
plt.xticks(rotation = 0)
plt.ylabel('Total ratings')
plt.show()

"""**Text Preprocessing**

*The following cells provide the steps taken to preprocess the review texts for better feature extraction. Once the text has been preprocessed, it can then be used to develop a vocabulary for training.*

1. Removing punctuations
2. Removing non alphabetical words or Removing digits
3. Lowercasing all words
4. Removing stopwords
5. Lemmatization to reduce words to their base form
6. Stemming
"""

#Text Preprocessing
import string
import nltk
nltk.download('wordnet')
nltk.download('stopwords')
nltk.download('omw-1.4')
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer

#created a dataframe that only contains the reviewText and the overall scoring of item
reviews = df[['reviewText', 'overall']]
reviews.head()

print('Original Text: ' + str(reviews['reviewText'][1]))
print('\n')

#create an empty mapping table from the str object to strip punctuation from the words
punc = str.maketrans('', '', string.punctuation)
#apply the empty mapping table to each element of the series where x is the review for one document.
reviews['reviewText'] = reviews['reviewText'].apply(lambda x : ' '.join(word.translate(punc) for word in x.split()))
print('Punctuation Remove: ' + str(reviews['reviewText'][1]))
print('\n')

#removing words that is non alpha
reviews['reviewText'] = reviews['reviewText'].apply(lambda x: ' '.join(word for word in x.split() if word.isalpha()))
print('Alphabetical Words: '+ str(reviews['reviewText'][1]))
print('\n')

#making all words to be lowercase
reviews['reviewText'] = reviews['reviewText'].apply(lambda x: ' '.join(word.lower() for word in x.split()))
print('Lowercase Words : '+ str(reviews['reviewText'][1]))
print('\n')

#list of stop words
stop = stopwords.words('english')
#removing the stop words
reviews['reviewText'] = reviews['reviewText'].apply(lambda x : ' '.join(word for word in x.split() if word not in stop))
print('Stopwords Remove: ' + str(reviews['reviewText'][1]))
print('\n')

#Lemmatize words to reduce them to their root form. Note: added the pos = 'v' to reduce the incoming word to verb root
lem = WordNetLemmatizer()
reviews['reviewText'] = reviews['reviewText'].apply(lambda x : ' '.join(lem.lemmatize(word, pos = 'v') for word in x.split()))
print('Lemmatized Text: ' + str(reviews['reviewText'][1]))

#separating the ratings to different sentiment
r1 = reviews[reviews.overall.isin([3,4,5])]
r0 = reviews[reviews.overall.isin([1,2])]
r1.loc[:, 'overall'] = 1
r0.loc[:, 'overall'] = 0
#concat the two new dataframes return one dataframe with preprocessed text and their corresponding labels
rev = pd.concat([r1,r0])
rev.head()

"""**Model training and tuning**"""

from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import LinearSVC
from sklearn.model_selection import train_test_split, cross_val_score,GridSearchCV, RandomizedSearchCV
from sklearn.metrics import confusion_matrix, classification_report, roc_curve, roc_auc_score
#ignore the warnings from sklearn
import warnings
warnings.filterwarnings('ignore')

"""**Multinomial Naive Bayes**"""

rev.overall.value_counts()

#balancing the training data by downsampling to the minority class given there is a good amount of data
def make_xy(data, vec, n):
    temp = pd.DataFrame()
    #sampling only n class reviews per class
    for rating in range(2):
        temp = pd.concat([temp, data[data.overall == rating].sample(n, random_state = 42)], ignore_index = True)
    #vectorizing the vocabulary
    X = vec.fit_transform(temp.reviewText)
    y = temp.overall
    return X, y

"""**Text vectorization**"""

#using CountVectorizer
count = CountVectorizer()
X, y = make_xy(rev, count, 20000)

#using TfidfVectorizer
tfidf = TfidfVectorizer()
Xt, yt = make_xy(rev, tfidf, 20000)

"""**Naive Bayes**"""

#testing the model with CountVectorizer
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3, random_state = 42, stratify = y)

naive = MultinomialNB()
naive.fit(X_train, y_train)
print('Training Accuracy w/ CountVectorizer: {:.2f}'.format(naive.score(X_train, y_train)))
print('Testing Accuracy w/ CountVectorizer: {:.2f}'.format(naive.score(X_test, y_test)))

#testing the model with tfidf vectorizer
Xt_train, Xt_test, yt_train, yt_test = train_test_split(Xt, yt, test_size = .3, random_state = 42, stratify = y)

nb = MultinomialNB()
nb.fit(Xt_train, yt_train)
print('Training Accuracy w/ TfidfVectorizer: {:.2f}'.format(nb.score(Xt_train, yt_train)))
print('Testing Accuracy w/ TfidfVectorizer: {:.2f}'.format(nb.score(Xt_test, yt_test)))

X.shape

"""**Hyperparameter Tuning on Naive Bayes**"""

#Tuning the min_df parameter for the vectorizer and the alpha in the multinomial Naive Bayes
best_alpha = 0
best_min_df = 0
best_score = 0

#param_grid
alphas = [.1, 1, 5, 10, 50]
min_dfs = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1]

#iterate throughout the param grid
for alpha in alphas:
    for m_df in min_dfs:
        tfidf = TfidfVectorizer(min_df = m_df)
        X, y = make_xy(rev, tfidf, 20000)
        naive = MultinomialNB(alpha = alpha)
        score = np.mean(cross_val_score(naive, X, y, scoring = 'accuracy', cv = 3))
        if score > best_score:
            best_score = score
            best_alpha = alpha
            best_min_df = m_df

print('Best_score: {:.2f}'.format(best_score))
print('Best_alpha: {:.2f}'.format(best_alpha))
print('Best_min_df: {:.5f}'.format(best_min_df))

tfidf = TfidfVectorizer(min_df = .001)
X, y = make_xy(rev,tfidf, 20000)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3, random_state = 42, stratify = y)
naive = MultinomialNB(alpha = 5)
naive.fit(X_train, y_train)
naive_pred = naive.predict(X_test)
naive_prob = naive.predict_proba(X_test)[:,1]
print('Training Accuracy w/ TfidfVectorizer: {:.2f}'.format(naive.score(X_train, y_train)))
print('Testing Accuracy w/ TfidfVectorizer: {:.2f}'.format(naive.score(X_test, y_test)))
print('\n')
print(confusion_matrix(y_test, naive_pred))
print('\n')
print(classification_report(y_test, naive_pred))

X_train.shape

"""**Logistic Regression**"""

#creating a grid space
c_space = np.logspace(-5,6, 15)
penalty = ['l1', 'l2']
param_grid = {'C':c_space, 'penalty': penalty}
logreg = LogisticRegression()

#exhaustive search for the best parameters
logreg_cv = GridSearchCV(logreg, param_grid, cv = 5)
logreg_cv.fit(X_train,y_train)

#train model using the best parameters determine from GridSearchCV
logreg = LogisticRegression(C = logreg_cv.best_params_['C'], penalty = logreg_cv.best_params_['penalty'])
logreg.fit(X_train, y_train)
logreg_pred = logreg.predict(X_test)
logreg_prob = logreg.predict_proba(X_test)[:,1]

print('Training Accuracy w/ TfidfVectorizer: {:.2f}'.format(logreg.score(X_train, y_train)))
print('Testing Accuracy w/ TfidfVectorizer: {:.2f}'.format(logreg.score(X_test, y_test)))
print('\n')
print(confusion_matrix(y_test, logreg_pred))
print('\n')
print(classification_report(y_test, logreg_pred))

"""**Support Vector Classifier**"""

#creating array of regularization values
c_space = np.logspace(-5,6, 8)
best_score = []
score = 0
best_c = 0

#exhaustive search for the best parameters
for value in c_space:
    svm = LinearSVC(C = value)
    score = np.mean(cross_val_score(naive, X_train, y_train, scoring = 'accuracy', cv = 5))
    if score > best_score:
        best_score = score
        best_c = value

#train model using the best parameters determine from kfold
try:
    svm = LinearSVC(best_c)
    svm.fit(X_train, y_train)
    svm_pred = svm.predict(X_test)

except:
    svm = LinearSVC()
    svm.fit(X_train, y_train)
    svm_pred = svm.predict(X_test)

print('Training Accuracy w/ TfidfVectorizer: {:.2f}'.format(svm.score(X_train, y_train)))
print('Testing Accuracy w/ TfidfVectorizer: {:.2f}'.format(svm.score(X_test, y_test)))
print('\n')
print(confusion_matrix(y_test, svm_pred))
print('\n')
print(classification_report(y_test, svm_pred))

"""**Random Forest Classifier**"""

#Random Forest Classifier
rf = RandomForestClassifier()
#param_grid
rf_params = {'n_estimators':[100, 500, 1000, 1500, 2000],
             'max_depth': [5, 10, 20, 30 ,40],
             'max_features': ['sqrt', 'log2'],
            'min_samples_leaf': [10, 100, 500, 1000, 2000],
            'min_samples_split': [200, 500, 1000, 2000]}

#fitting the randomize search
rf_cv = RandomizedSearchCV(rf, rf_params, cv = 3, n_jobs = -1)
rf_cv.fit(X_train,y_train)

"""**Finalizing results with best model**"""

best_model = rf_cv.best_estimator_
best_model.fit(X_train, y_train)
rf_pred = best_model.predict(X_test)
rf_prob = best_model.predict_proba(X_test)[:,1]
print(rf_cv.best_params_)
print('Training Accuracy w/ TfidfVectorizer: {:.2f}'.format(best_model.score(X_train, y_train)))
print('Testing Accuracy w/ TfidfVectorizer: {:.2f}'.format(best_model.score(X_test, y_test)))
print('\n')
print(confusion_matrix(y_test, rf_pred))
print('\n')
print(classification_report(y_test, rf_pred))

"""#Performance comparison

**Plotting the roc curves for each model**
"""

import seaborn as sns
sns.set_style('white')
sns.set_style('white')
fig, ax = plt.subplots()
ax.plot([0,1], [0,1], linestyle = '--', color = 'darkorange')

probs = [naive_prob, logreg_prob, rf_prob, svm_pred]
labels = ['Naive Bayes', 'Logistic Regression', 'Random Forest', 'SVC']
for idx in range(len(probs)):
    fpr, tpr, thresholds = roc_curve(y_test, probs[idx])
    ax.plot(fpr, tpr, label = (labels[idx] + ' AUC score = %.2f' % roc_auc_score(y_test, probs[idx])))

plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curves')
ax.legend(loc = 'lower right')
ax.margins(x = 0.01, y =0.02)


plt.show()

"""**Streamlit dashboard**"""

!pip install streamlit pyngrok

import streamlit as st

# Path to the logo image
logo_path = "/content/Logo.png"

# Display the logo at the top of the sidebar
st.sidebar.image(logo_path, use_column_width=True)

# Example content in the main area
st.title("Main Content Area")
st.write("This is the main content area of the Streamlit app.")

import streamlit as st

# Path to the logo image
logo_path = "/content/Logo.png"

# Display the logo at the top of the sidebar
st.sidebar.image(logo_path, use_column_width=True)

# Example content in the main area
st.title("Main Content Area")
st.write("This is the main content area of the Streamlit app.")

from pyngrok import ngrok #secure tunnel create
# Set your Ngrok authtoken
ngrok.set_auth_token("2sBIK4OfMHSagEGgP3KwEhcfS95_3hwyy2xE5R8MACcVfB6vE")

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import pandas as pd
# import streamlit as st
# import plotly.express as px
# 
# # Set the title of the app with an emoji
# st.title("Customer Feedback Analysis üñç")
# 
# # Sidebar: Adding logo at the top
# logo_path = "/content/Logo.png"
# st.sidebar.image(logo_path, use_column_width=True)
# 
# # File upload widget with custom styling and emoji
# uploaded_file = st.file_uploader("Upload a CSV file üìÇ", type=["csv"], label_visibility="collapsed")
# 
# if uploaded_file is not None:
#     # Load the dataset
#     df = pd.read_csv(uploaded_file)
# 
#     # Display a beautifully styled data preview with emoji
#     st.write("### Data Preview üëÄ")
#     st.dataframe(df.head(), height=300, width=600)
# 
#     # Sidebar: Adding some decoration, custom background color, and emojis
#     st.sidebar.header("Analysis Options üîç")
#     st.sidebar.markdown("""
#         <style>
#         .sidebar .sidebar-content {
#             background-color: #f1f1f1;
#             border-radius: 15px;
#         }
#         </style>
#     """, unsafe_allow_html=True)
# 
#     # Sidebar selections for analysis with emojis
#     columns = df.columns.tolist()
#     x_axis = st.sidebar.selectbox("Select X-axis üìä", columns)
#     y_axis = st.sidebar.selectbox("Select Y-axis üîÅ", columns)
#     color = st.sidebar.selectbox("Select Column for Coloring üé®", columns, index=0)
# 
#     # Data preprocessing summary with a styled table and emoji
#     st.write("### Summary Statistics üìä:")
#     st.table(df.describe())
# 
#     # Distribution Plot with emoji
#     st.subheader("Distribution Plot üïΩ")
#     selected_column = st.selectbox("Select a column for distribution analysis", columns)
#     dist_chart = px.histogram(df, x=selected_column, nbins=20, title=f"Distribution of {selected_column}", color_discrete_sequence=["#FF6347"])
#     st.plotly_chart(dist_chart)
# 
#     # Scatter Plot with added hover data and updated color
#     st.subheader("Scatter Plot ‚ú®")
#     scatter_chart = px.scatter(
#         df,
#         x=x_axis,
#         y=y_axis,
#         color=color,
#         title=f"{y_axis} vs {x_axis}",
#         labels={x_axis: x_axis, y_axis: y_axis},
#         hover_data=[x_axis, y_axis, color],
#         color_discrete_sequence=["#1f77b4"]
#     )
#     st.plotly_chart(scatter_chart)
# 
#     # Bar Chart with customized bar colors and emoji
#     st.subheader("Bar Chart üìä")
#     bar_x = st.selectbox("Select a column for bar chart (categorical)", columns)
#     bar_chart = px.bar(
#         df,
#         x=bar_x,
#         title=f"Bar Chart of {bar_x}",
#         labels={bar_x: bar_x, "count": "Count"},
#         color_discrete_sequence=["#32CD32"]
#     )
#     st.plotly_chart(bar_chart)
# 
#     # Box Plot with emoji
#     st.subheader("Box Plot üì¶")
#     box_chart = px.box(
#         df,
#         x=x_axis,
#         y=y_axis,
#         color=color,
#         title=f"Box Plot of {y_axis} by {x_axis}",
#         color_discrete_sequence=["#FF6347"]
#     )
#     st.plotly_chart(box_chart)
# 
#     # Correlation heatmap with styled color scale and emoji
#     st.subheader("Correlation Heatmap üå°Ô∏è")
#     numeric_df = df.select_dtypes(include=["float", "int"])
#     if numeric_df.shape[1] > 1:
#         correlation = numeric_df.corr()
#         heatmap = px.imshow(
#             correlation,
#             text_auto=True,
#             title="Correlation Heatmap",
#             labels=dict(color="Correlation"),
#             color_continuous_scale="Viridis"
#         )
#         st.plotly_chart(heatmap)
#     else:
#         st.write("Not enough numerical columns for correlation heatmap. üö´")
# 
# else:
#     st.info("Please upload a CSV file to get started. üì•")

# Run Streamlit and expose it publicly via ngrok
!streamlit run app.py &>/dev/null &

from pyngrok import ngrok

# Connect to the Streamlit server running on port 8501
public_url = ngrok.connect(addr=8501, bind_tls=True)

# Print the public URL for accessing the Streamlit app
print(f"Streamlit is running at: {public_url}")